{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from os.path import expanduser\n",
    "import json\n",
    "import alphatims.bruker\n",
    "import os\n",
    "import numpy as np\n",
    "from ms_deisotope import deconvolute_peaks, averagine, scoring\n",
    "from ms_deisotope.deconvolution import peak_retention_strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mass of a proton in unified atomic mass units, or Da. For calculating the monoisotopic mass.\n",
    "PROTON_MASS = 1.00727647"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = 'P3856_YHE211'\n",
    "run_name = 'P3856_YHE211_1_Slot1-1_1_5104'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### precursor cuboids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfde_results_base_dir = '/media/data-4t-a/results-P3856_YHE211/2021-10-06-06-59-25/P3856_YHE211'\n",
    "precursor_cuboids_name = '{}/precursor-cuboids-pasef/exp-{}-run-{}-precursor-cuboids-pasef.feather'.format(tfde_results_base_dir,experiment_name,run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "precursor_cuboids_df = pd.read_feather(precursor_cuboids_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3did features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading features from /media/big-ssd/results-P3856-3did/minvi-2000-2021-11-30-17-20-22/features-3did/exp-P3856-run-P3856_YHE211_1_Slot1-1_1_5104-features-3did-ident.feather\n"
     ]
    }
   ],
   "source": [
    "tdid_experiment_name = 'P3856'\n",
    "tdid_results_name = 'minvi-2000-2021-11-30-17-20-22'\n",
    "features_dir = '/media/big-ssd/results-{}-3did/{}/features-3did'.format(tdid_experiment_name, tdid_results_name)\n",
    "features_file = '{}/exp-{}-run-{}-features-3did-dedup.feather'.format(features_dir, tdid_experiment_name, run_name, suffix_to_process)\n",
    "print('loading features from {}'.format(features_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = pd.read_feather(features_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "165663"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_features_detected = len(features_df)\n",
    "number_features_detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "charge range: 1 to 8\n"
     ]
    }
   ],
   "source": [
    "min_charge = features_df.charge.min()\n",
    "max_charge = features_df.charge.max()\n",
    "print('charge range: {} to {}'.format(min_charge,max_charge))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ms2 raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_DATABASE_BASE_DIR = '/media/big-ssd/experiments/{}/raw-databases'.format(experiment_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading raw data from /media/big-ssd/experiments/P3856_YHE211/raw-databases/P3856_YHE211_1_Slot1-1_1_5104.hdf\n"
     ]
    }
   ],
   "source": [
    "# create the TimsTOF object\n",
    "RAW_HDF_FILE = '{}.hdf'.format(run_name)\n",
    "RAW_HDF_PATH = '{}/{}'.format(RAW_DATABASE_BASE_DIR, RAW_HDF_FILE)\n",
    "if not os.path.isfile(RAW_HDF_PATH):\n",
    "    print('{} doesn\\'t exist so loading the raw data from {}'.format(RAW_HDF_PATH, RAW_DATABASE_NAME))\n",
    "    data = alphatims.bruker.TimsTOF(RAW_DATABASE_NAME)\n",
    "    print('saving to {}'.format(RAW_HDF_PATH))\n",
    "    _ = data.save_as_hdf(\n",
    "        directory=RAW_DATABASE_BASE_DIR,\n",
    "        file_name=RAW_HDF_FILE,\n",
    "        overwrite=True\n",
    "    )\n",
    "else:\n",
    "    print('loading raw data from {}'.format(RAW_HDF_PATH))\n",
    "    data = alphatims.bruker.TimsTOF(RAW_HDF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "INSTRUMENT_RESOLUTION = 40000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find 3sigma for a specified m/z\n",
    "def calculate_peak_delta(mz):\n",
    "    delta_m = mz / INSTRUMENT_RESOLUTION  # FWHM of the peak\n",
    "    sigma = delta_m / 2.35482  # std dev is FWHM / 2.35482. See https://en.wikipedia.org/wiki/Full_width_at_half_maximum\n",
    "    peak_delta = 3 * sigma  # 99.7% of values fall within +/- 3 sigma\n",
    "    return peak_delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the intensity-weighted centroid\n",
    "# takes a numpy array of intensity, and another of mz\n",
    "def intensity_weighted_centroid(_int_f, _x_f):\n",
    "    return ((_int_f/_int_f.sum()) * _x_f).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# peaks_a is a numpy array of [mz,intensity]\n",
    "# returns a numpy array of [intensity_weighted_centroid,summed_intensity]\n",
    "def intensity_descent(peaks_a, peak_delta=None):\n",
    "    # intensity descent\n",
    "    peaks_l = []\n",
    "    while len(peaks_a) > 0:\n",
    "        # find the most intense point\n",
    "        max_intensity_index = np.argmax(peaks_a[:,1])\n",
    "        peak_mz = peaks_a[max_intensity_index,0]\n",
    "        if peak_delta == None:\n",
    "            peak_delta = calculate_peak_delta(mz=peak_mz)\n",
    "        peak_mz_lower = peak_mz - peak_delta\n",
    "        peak_mz_upper = peak_mz + peak_delta\n",
    "\n",
    "        # get all the raw points within this m/z region\n",
    "        peak_indexes = np.where((peaks_a[:,0] >= peak_mz_lower) & (peaks_a[:,0] <= peak_mz_upper))[0]\n",
    "        if len(peak_indexes) > 0:\n",
    "            mz_cent = intensity_weighted_centroid(peaks_a[peak_indexes,1], peaks_a[peak_indexes,0])\n",
    "            summed_intensity = peaks_a[peak_indexes,1].sum()\n",
    "            peaks_l.append((mz_cent, summed_intensity))\n",
    "            # remove the raw points assigned to this peak\n",
    "            peaks_a = np.delete(peaks_a, peak_indexes, axis=0)\n",
    "    return np.array(peaks_l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resolve the fragment ions for this feature\n",
    "# returns a decharged peak list (neutral mass+proton mass, intensity)\n",
    "def resolve_fragment_ions(feature_charge, ms2_points_df):\n",
    "    # perform intensity descent to resolve peaks\n",
    "    raw_points_a = ms2_points_df[['mz','intensity']].to_numpy()\n",
    "    peaks_a = intensity_descent(peaks_a=raw_points_a, peak_delta=None)\n",
    "    \n",
    "    # deconvolution\n",
    "    # for details on deconvolute_peaks see https://mobiusklein.github.io/ms_deisotope/docs/_build/html/deconvolution/deconvolution.html\n",
    "    # returns a list of DeconvolutedPeak - see https://github.com/mobiusklein/ms_deisotope/blob/bce522a949579a5f54465eab24194eb5693f40ef/ms_deisotope/peak_set.py#L78\n",
    "    peaks_l = list(map(tuple, peaks_a))\n",
    "    maximum_neutral_mass = 1700*feature_charge  # give the deconvolution a reasonable upper limit to search within\n",
    "    deconvoluted_peaks, _ = deconvolute_peaks(peaks_l, use_quick_charge=True, averagine=averagine.peptide, scorer=scoring.PenalizedMSDeconVFitter(minimum_score=20., penalty_factor=3.0), truncate_after=0.95, ignore_below=0.0, charge_range=(1,feature_charge), retention_strategy=peak_retention_strategy.TopNRetentionStrategy(n_peaks=100, base_peak_coefficient=1e-6, max_mass=maximum_neutral_mass))\n",
    "    \n",
    "    # package the spectra as a dataframe\n",
    "    deconvoluted_peaks_l = []\n",
    "    for peak in deconvoluted_peaks:\n",
    "        d = {}\n",
    "        d['singly_protonated_mass'] = round(peak.neutral_mass+PROTON_MASS, 4)\n",
    "        d['neutral_mass'] = round(peak.neutral_mass, 4)\n",
    "        d['intensity'] = peak.intensity\n",
    "        deconvoluted_peaks_l.append(d)\n",
    "    deconvoluted_peaks_df = pd.DataFrame(deconvoluted_peaks_l)\n",
    "    \n",
    "    # sort and normalise intensity\n",
    "    deconvoluted_peaks_df.sort_values(by=['intensity'], ascending=False, inplace=True)\n",
    "    deconvoluted_peaks_df.intensity = deconvoluted_peaks_df.intensity / deconvoluted_peaks_df.intensity.max() * 1000.0\n",
    "    deconvoluted_peaks_df.intensity = deconvoluted_peaks_df.intensity.astype(np.uint)\n",
    "    deconvoluted_peaks_df = deconvoluted_peaks_df[(deconvoluted_peaks_df.intensity > 0)]\n",
    "    \n",
    "    return deconvoluted_peaks_df.head(n=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_with_fragments_l = []\n",
    "for cuboid in precursor_cuboids_df.itertuples():\n",
    "    # determine the ms1 extent of the precursor cuboid\n",
    "    mz_lower = cuboid.wide_mz_lower\n",
    "    mz_upper = cuboid.wide_mz_upper\n",
    "    rt_lower = cuboid.wide_ms1_rt_lower\n",
    "    rt_upper = cuboid.wide_ms1_rt_upper\n",
    "    scan_lower = cuboid.wide_scan_lower\n",
    "    scan_upper = cuboid.wide_scan_upper\n",
    "\n",
    "    # load the ms2 points for this cuboid's fragmentation event\n",
    "    ms2_points_df = data[\n",
    "        {\n",
    "            \"frame_indices\": slice(int(cuboid.fe_ms2_frame_lower), int(cuboid.fe_ms2_frame_upper+1)),\n",
    "            \"scan_indices\": slice(int(cuboid.fe_scan_lower), int(cuboid.fe_scan_upper+1)),\n",
    "            \"precursor_indices\": slice(1, None)  # ms2 frames only\n",
    "        }\n",
    "    ][['mz_values','scan_indices','frame_indices','rt_values','intensity_values']]\n",
    "    ms2_points_df.rename(columns={'mz_values':'mz', 'scan_indices':'scan', 'frame_indices':'frame_id', 'rt_values':'retention_time_secs', 'intensity_values':'intensity'}, inplace=True)\n",
    "\n",
    "    # downcast the data types to minimise the memory used\n",
    "    int_columns = ['frame_id','scan','intensity']\n",
    "    ms2_points_df[int_columns] = ms2_points_df[int_columns].apply(pd.to_numeric, downcast=\"unsigned\")\n",
    "    float_columns = ['retention_time_secs']\n",
    "    ms2_points_df[float_columns] = ms2_points_df[float_columns].apply(pd.to_numeric, downcast=\"float\")\n",
    "\n",
    "    # get all the 3did features with an apex within the cuboid's bounds\n",
    "    features_subset_df = features_df[(features_df.monoisotopic_mz >= mz_lower) & (features_df.monoisotopic_mz <= mz_upper) & (features_df.rt_apex >= rt_lower) & (features_df.rt_apex <= rt_upper) & (features_df.scan_apex >= scan_lower) & (features_df.scan_apex <= scan_upper)].copy()\n",
    "    \n",
    "    if len(features_subset_df) > 0:\n",
    "        # add the precursor identifier\n",
    "        features_subset_df['precursor_cuboid_id'] = cuboid.precursor_cuboid_id\n",
    "\n",
    "        # resolve the fragment ions for this feature's charge\n",
    "        features_subset_df['fragment_ions_l'] = features_subset_df.apply(lambda row: json.dumps(resolve_fragment_ions(row.charge, ms2_points_df).to_dict(orient='records')), axis=1)\n",
    "        \n",
    "        # add these features to the list\n",
    "        features_with_fragments_l.append(features_subset_df)\n",
    "\n",
    "# join the list of dataframes into a single dataframe\n",
    "features_within_fragments_df = pd.concat(features_with_fragments_l, axis=0, sort=False, ignore_index=True)\n",
    "\n",
    "# add the run name\n",
    "features_within_fragments_df['run_name'] = run_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save it back to the de-dup file because that's what the next step expects\n",
    "features_within_fragments_df.to_feather('/media/big-ssd/experiments/P3856_YHE211/features-3did/exp-P3856_YHE211-run-P3856_YHE211_1_Slot1-1_1_5104-features-3did-dedup.feather')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "828 features\n"
     ]
    }
   ],
   "source": [
    "print('{} features'.format(len(features_within_fragments_df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 unique features inside isolation windows, 0.0% of features detected\n"
     ]
    }
   ],
   "source": [
    "number_features_inside_isol_windows = len(features_within_fragments_df.feature_id.unique())\n",
    "print('{} unique features inside isolation windows, {}% of features detected'.format(number_features_inside_isol_windows, round(number_features_inside_isol_windows/number_features_detected*100.0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
